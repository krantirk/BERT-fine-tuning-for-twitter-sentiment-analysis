{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features=10000\n",
    "maxlen=500\n",
    "embedding_dim=128\n",
    "(x_train,y_train),(x_test,y_test)=imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=pad_sequences(x_train,maxlen=maxlen)\n",
    "x_test=pad_sequences(x_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500) (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 128)     1280000     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 498, 200)     77000       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 497, 200)     102600      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 496, 200)     128200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 200)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 200)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 200)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 600)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 30)        18030       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 30)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 30)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            31          flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,605,861\n",
      "Trainable params: 1,605,861\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "num_filters=200 \n",
    "#模型定义\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_input=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_features,embedding_dim,input_length=maxlen)(text_input)\n",
    "#三种卷积[3,4,5]\n",
    "conv1_3=layers.Conv1D(num_filters,3,activation='relu')(embedded_text)\n",
    "conv1_4=layers.Conv1D(num_filters,4,activation='relu')(embedded_text)\n",
    "conv1_5=layers.Conv1D(num_filters,5,activation='relu')(embedded_text)\n",
    "#最大池化\n",
    "maxpool_3=layers.MaxPool1D(maxlen-3+1)(conv1_3)\n",
    "maxpool_4=layers.MaxPool1D(maxlen-4+1)(conv1_4)\n",
    "maxpool_5=layers.MaxPool1D(maxlen-5+1)(conv1_5)\n",
    "#拼接\n",
    "concatenated=layers.concatenate([maxpool_3,maxpool_4,maxpool_5],axis=-1)\n",
    "#全连接层\n",
    "x = layers.Dense(30, activation='relu')(concatenated)\n",
    "# #dropout\n",
    "x=layers.Dropout(0.5)(x)\n",
    "# #平铺\n",
    "x=layers.Flatten()(x)\n",
    "#分类\n",
    "output=layers.Dense(1,activation='sigmoid')(x)\n",
    "model_conv=Model(text_input,output)\n",
    "model_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 22s 898us/step - loss: 0.5256 - acc: 0.7229 - val_loss: 0.3031 - val_acc: 0.8752\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 20s 791us/step - loss: 0.2864 - acc: 0.8991 - val_loss: 0.2507 - val_acc: 0.8977\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 20s 793us/step - loss: 0.1855 - acc: 0.9472 - val_loss: 0.2717 - val_acc: 0.8948\n"
     ]
    }
   ],
   "source": [
    "model_conv.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "history_conv=model_conv.fit(x_train,y_train,epochs=3,batch_size=128,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               183200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                6030      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 1,469,261\n",
      "Trainable params: 1,469,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_input=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_features,embedding_dim,input_length=maxlen)(text_input)\n",
    "x=layers.Bidirectional(layers.LSTM(100,activation='tanh',return_sequences=False, dropout=0.5, recurrent_dropout=0.1))(embedded_text)\n",
    "x=layers.Dropout(0.5)(x)\n",
    "x=layers.Dense(30,activation='relu')(x)\n",
    "output=layers.Dense(1,activation='sigmoid')(x)\n",
    "model_biLSTM=Model(text_input,output)\n",
    "model_biLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 19s 765us/step - loss: 0.1222 - acc: 0.9698 - val_loss: 0.3028 - val_acc: 0.8887\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 19s 762us/step - loss: 0.0788 - acc: 0.9849 - val_loss: 0.3624 - val_acc: 0.8905\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 19s 764us/step - loss: 0.0533 - acc: 0.9914 - val_loss: 0.3934 - val_acc: 0.8918\n"
     ]
    }
   ],
   "source": [
    "model_biLSTM.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "history_biLSTM=model_conv.fit(x_train,y_train,epochs=3,batch_size=128,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "class ConvInputLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Distribute word vectors into chunks - input for the convolution operation\n",
    "    Input dim: [batch_size x sentence_len x word_vec_dim]\n",
    "    Output dim: [batch_size x (sentence_len - filter_width + 1) x filter_width x word_vec_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_width, sent_len, **kwargs):\n",
    "        super(ConvInputLayer, self).__init__(**kwargs)\n",
    "        self.filter_width = filter_width\n",
    "        self.sent_len = sent_len\n",
    "\n",
    "    def call(self, x):\n",
    "        chunks = []\n",
    "        for i in range(self.sent_len - self.filter_width + 1):\n",
    "            chunk = x[:, i:i+self.filter_width, :]\n",
    "            chunk = K.expand_dims(chunk, 1)\n",
    "            chunks.append(chunk)\n",
    "        return K.concatenate(chunks, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.sent_len - self.filter_width + 1, self.filter_width, input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 500, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv_input_layer_1 (ConvInpu (None, 496, 5, 128)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 496, 300)          514800    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,795,101\n",
      "Trainable params: 1,795,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_width=5\n",
    "\n",
    "#以LSTM为卷积的filter\n",
    "input_lstm_filter=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_features,embedding_dim,input_length=maxlen)(input_lstm_filter)\n",
    "x=layers.Dropout(0.5)(embedded_text)\n",
    "emb_layer=ConvInputLayer(filter_width, maxlen)(x)\n",
    "conv_layer = layers.TimeDistributed(layers.LSTM(300, dropout=0.4, recurrent_dropout=0.4))(emb_layer)\n",
    "text_layer = layers.GlobalMaxPooling1D()(conv_layer)\n",
    "output=layers.Dense(1,activation='sigmoid')(text_layer)\n",
    "model_filterLSTM=Model(input_lstm_filter,output)\n",
    "model_filterLSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      "25000/25000 [==============================] - 19s 772us/step - loss: 0.0431 - acc: 0.9932 - val_loss: 0.4719 - val_acc: 0.8904\n",
      "Epoch 2/3\n",
      "25000/25000 [==============================] - 19s 764us/step - loss: 0.0368 - acc: 0.9942 - val_loss: 0.4973 - val_acc: 0.8911\n",
      "Epoch 3/3\n",
      "25000/25000 [==============================] - 19s 765us/step - loss: 0.0330 - acc: 0.9946 - val_loss: 0.5953 - val_acc: 0.8886\n"
     ]
    }
   ],
   "source": [
    "model_filterLSTM.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "history_filterLSTM=model_conv.fit(x_train,y_train,epochs=3,batch_size=128,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
