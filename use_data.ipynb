{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import string as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\"[!#$%&'()*+,-./:;<=>?@[\\]^_`{|}~]\",' ',string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data=list(open(filename,'r',encoding='utf-8').readlines())\n",
    "    labels=[]\n",
    "    x_text=[]\n",
    "    for t in data:\n",
    "        t=t.split('+++$+++')\n",
    "        labels.append(float(t[0].strip()))\n",
    "        x_text.append(clean_str(t[1]))\n",
    "    return labels,x_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 200000\n"
     ]
    }
   ],
   "source": [
    "labels,x_text=load_data('data/training_label.csv')\n",
    "print(len(labels),len(x_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(string):\n",
    "    words=[word for word in string.split(' ') if word not in stopwords.words('english')]\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thanks so much for your donation for chance ur da best     such a hard blow after working so hard to save him\n"
     ]
    }
   ],
   "source": [
    "print(x_text[230])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thanks much donation chance ur da best     hard blow working hard save\n"
     ]
    }
   ],
   "source": [
    "print(remove_stopwords(x_text[230]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "#输出最大的文本有多少词\n",
    "max_length=max([len(x.split(' ')) for x in x_text])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=120\n",
    "validation_samples=5000\n",
    "training_samples=len(labels)-validation_samples\n",
    "max_words=20000\n",
    "embedding_dim=200     #词向量的维度\n",
    "num_filters=200      #每种卷积的数量，3种卷积，共600个\n",
    "\n",
    "tokenizer=Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(x_text)\n",
    "sequences=tokenizer.texts_to_sequences(x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81088 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index=tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pad_sequences(sequences,maxlen=maxlen)\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (200000, 120)\n",
      "Shape of label tensor: (200000,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#划分训练集和验证集\n",
    "x_train=data[:training_samples]\n",
    "y_train=labels[:training_samples]\n",
    "\n",
    "x_val=data[training_samples:]\n",
    "y_val=labels[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195000, 120) (195000,) (5000, 120) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape,x_val.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 120, 100)     2000000     text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 118, 200)     60200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 117, 200)     80200       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 116, 200)     100200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 200)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 200)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 200)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 600)       0           max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "                                                                 max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1, 30)        18030       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1, 30)        0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 30)           0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            31          flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,258,661\n",
      "Trainable params: 2,258,661\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#模型定义\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "text_input=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_words,embedding_dim,input_length=maxlen)(text_input)\n",
    "#三种卷积[3,4,5]\n",
    "conv1_3=layers.Conv1D(num_filters,3,activation='relu')(embedded_text)\n",
    "conv1_4=layers.Conv1D(num_filters,4,activation='relu')(embedded_text)\n",
    "conv1_5=layers.Conv1D(num_filters,5,activation='relu')(embedded_text)\n",
    "#最大池化\n",
    "maxpool_3=layers.MaxPool1D(maxlen-3+1)(conv1_3)\n",
    "maxpool_4=layers.MaxPool1D(maxlen-4+1)(conv1_4)\n",
    "maxpool_5=layers.MaxPool1D(maxlen-5+1)(conv1_5)\n",
    "#拼接\n",
    "concatenated=layers.concatenate([maxpool_3,maxpool_4,maxpool_5],axis=-1)\n",
    "#全连接层\n",
    "x = layers.Dense(30, activation='relu')(concatenated)\n",
    "# #dropout\n",
    "x=layers.Dropout(0.5)(x)\n",
    "# #平铺\n",
    "x=layers.Flatten()(x)\n",
    "#分类\n",
    "output=layers.Dense(1,activation='sigmoid')(x)\n",
    "model=Model(text_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 120, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                6030      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 2,246,861\n",
      "Trainable params: 2,246,861\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#模型定义\n",
    "from keras.models import Model\n",
    "from keras import layers\n",
    "from keras import Input\n",
    "\n",
    "embedding_dim=200     #词向量的维度\n",
    "num_filters=200      #每种卷积的数量，3种卷积，共600个\n",
    "max_words=10000\n",
    "maxlen=120\n",
    "text_input=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_words,embedding_dim,input_length=maxlen)(text_input)\n",
    "x=layers.Bidirectional(layers.LSTM(100,activation='tanh',return_sequences=False, dropout=0.5, recurrent_dropout=0.1))(embedded_text)\n",
    "x=layers.Dropout(0.5)(x)\n",
    "x=layers.Dense(30,activation='relu')(x)\n",
    "output=layers.Dense(1,activation='sigmoid')(x)\n",
    "model=Model(text_input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 195000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "195000/195000 [==============================] - 38s 195us/step - loss: 0.4767 - acc: 0.7768 - val_loss: 0.4354 - val_acc: 0.7974\n",
      "Epoch 2/3\n",
      "195000/195000 [==============================] - 34s 177us/step - loss: 0.3840 - acc: 0.8326 - val_loss: 0.4443 - val_acc: 0.8030\n",
      "Epoch 3/3\n",
      "195000/195000 [==============================] - 35s 177us/step - loss: 0.2986 - acc: 0.8755 - val_loss: 0.5115 - val_acc: 0.7918\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=3,batch_size=128,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 195000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "195000/195000 [==============================] - 420s 2ms/step - loss: 0.4773 - acc: 0.7711 - val_loss: 0.4338 - val_acc: 0.7980\n",
      "Epoch 2/3\n",
      "195000/195000 [==============================] - 418s 2ms/step - loss: 0.4161 - acc: 0.8095 - val_loss: 0.4289 - val_acc: 0.8034\n",
      "Epoch 3/3\n",
      "195000/195000 [==============================] - 416s 2ms/step - loss: 0.3907 - acc: 0.8233 - val_loss: 0.4337 - val_acc: 0.8002\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=3,batch_size=128,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "class ConvInputLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    Distribute word vectors into chunks - input for the convolution operation\n",
    "    Input dim: [batch_size x sentence_len x word_vec_dim]\n",
    "    Output dim: [batch_size x (sentence_len - filter_width + 1) x filter_width x word_vec_dim]\n",
    "    \"\"\"\n",
    "    def __init__(self, filter_width, sent_len, **kwargs):\n",
    "        super(ConvInputLayer, self).__init__(**kwargs)\n",
    "        self.filter_width = filter_width\n",
    "        self.sent_len = sent_len\n",
    "\n",
    "    def call(self, x):\n",
    "        chunks = []\n",
    "        for i in range(self.sent_len - self.filter_width + 1):\n",
    "            chunk = x[:, i:i+self.filter_width, :]\n",
    "            chunk = K.expand_dims(chunk, 1)\n",
    "            chunks.append(chunk)\n",
    "        return K.concatenate(chunks, 1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.sent_len - self.filter_width + 1, self.filter_width, input_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text (InputLayer)            (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 120, 200)          2000000   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 120, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv_input_layer_3 (ConvInpu (None, 116, 5, 200)       0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 116, 300)          601200    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 2,601,501\n",
      "Trainable params: 2,601,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "filter_width=5\n",
    "\n",
    "#以LSTM为卷积的filter\n",
    "input_lstm_filter=Input(shape=(None,),dtype='int32',name='text')\n",
    "embedded_text=layers.Embedding(max_words,embedding_dim,input_length=maxlen)(input_lstm_filter)\n",
    "x=layers.Dropout(0.5)(embedded_text)\n",
    "emb_layer=ConvInputLayer(filter_width, maxlen)(x)\n",
    "conv_layer = layers.TimeDistributed(layers.LSTM(300, dropout=0.4, recurrent_dropout=0.4))(emb_layer)\n",
    "text_layer = layers.GlobalMaxPooling1D()(conv_layer)\n",
    "output=layers.Dense(1,activation='sigmoid')(text_layer)\n",
    "model=Model(input_lstm_filter,output)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 195000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "195000/195000 [==============================] - 229s 1ms/step - loss: 0.4878 - acc: 0.7621 - val_loss: 0.4430 - val_acc: 0.7908\n",
      "Epoch 2/3\n",
      "195000/195000 [==============================] - 230s 1ms/step - loss: 0.4285 - acc: 0.8022 - val_loss: 0.4353 - val_acc: 0.7962\n",
      "Epoch 3/3\n",
      "195000/195000 [==============================] - 230s 1ms/step - loss: 0.4075 - acc: 0.8141 - val_loss: 0.4339 - val_acc: 0.8006\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
    "history=model.fit(x_train,y_train,epochs=3,batch_size=128,validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
